{"cells":[{"cell_type":"markdown","metadata":{"id":"tggEl6a8IjHx"},"source":["import packages\n","------------------\n","\n","Some packages are installed automatically if you use Anaconda. As pytorch is used here, you are expected to install that in your machine. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UN61B7PL_7wQ"},"outputs":[],"source":["# previous verions: Copy of step002_det_P002_segDistTransform_v9_eval_2022_02_09.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17203,"status":"ok","timestamp":1675707987635,"user":{"displayName":"Jennifer Feng","userId":"02244244752550378593"},"user_tz":360},"id":"0Dzmo4dOJcTl","outputId":"286b5c80-0b2a-4dcf-90ca-b203abeff5e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","# drive.flush_and_unmount()\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dyq8L-NBKPWv"},"outputs":[],"source":["# added to be able to run in Google Colab\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/UTRECHT/utils')\n","sys.path.insert(0,'/content/gdrive/MyDrive/UTRECHT')\n","\n","import utils"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8180,"status":"ok","timestamp":1675707997221,"user":{"displayName":"Jennifer Feng","userId":"02244244752550378593"},"user_tz":360},"id":"7m1Ta0akIjIA","outputId":"e3f2758b-5717-41b3-ead6-d41280d26da9"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.8.10 (default, Nov 14 2022, 12:59:47) \n","[GCC 9.4.0]\n","1.13.1+cu116\n"]}],"source":["from __future__ import print_function, division\n","import os, random, time, copy\n","from skimage import io, transform, morphology, feature, measure\n","from skimage.morphology import dilation, square, binary_opening\n","from skimage.draw import rectangle_perimeter\n","import numpy as np\n","import os.path as path\n","import scipy.io as sio\n","import scipy\n","from scipy import misc\n","from scipy import ndimage, signal\n","from scipy.ndimage import gaussian_filter\n","import pickle\n","import sys\n","import math\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import PIL.Image\n","from io import BytesIO\n","from skimage import data, img_as_float\n","from skimage.metrics import structural_similarity as ssim\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","import pandas as pd \n","import matplotlib.patheffects as path_effects\n","\n","from skimage.filters import threshold_otsu\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler \n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torchvision\n","from torchvision import datasets, models, transforms\n","\n","# sys.path.append('/home/skong2/project/dpff4ldl')\n","# from utils.metrics import *\n","# from losses import *\n","\n","from utils.flow_functions import *\n","from utils.dataset import *\n","from utils.network_arch import *\n","from utils.trainval_detSegDistTransform import *\n","\n","import warnings # ignore warnings\n","warnings.filterwarnings(\"ignore\")\n","print(sys.version)\n","print(torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"E8xdQF1zIjIT"},"source":["\n","\n","Evaluation and Visualization\n","-----"]},{"cell_type":"code","source":["#### set project name, save directory for the project\n","\n","exp_dir = '/content/gdrive/MyDrive/exp/05_Exp3_ContinuousTraining' \n","\n","trial_list = ['Trial_01', 'Trial_02','Trial_03','Trial_04','Trial_05']\n","time_step_list =['gen','T0','T1','T2','Full']\n","trial = trial_list[3]    # set to the trial number\n","time_step = time_step_list[1] # set to time step number\n","t_minus_1_time_step = time_step_list[0] # set to previous time step\n","\n","save_dir = os.path.join(exp_dir, trial, time_step) \n","t_minus_1_save_dir = '/content/gdrive/MyDrive/exp/01_Generalized_Model_FullTrainingData/Trial_01'\n","\n","eval_dir = '/content/gdrive/MyDrive/eval/05_Exp3_ContinuousTraining' \n","eval_dir = os.path.join(eval_dir, trial, time_step) \n","if not os.path.exists(eval_dir): \n","    os.makedirs(eval_dir)\n","\n","print(time_step)\n","print(t_minus_1_time_step)\n","print(eval_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PAmvki6JFKZ","executionInfo":{"status":"ok","timestamp":1675707997601,"user_tz":360,"elapsed":392,"user":{"displayName":"Jennifer Feng","userId":"02244244752550378593"}},"outputId":"0f56380e-3db6-486a-fa79-77e4732cfe1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["T0\n","gen\n","/content/gdrive/MyDrive/eval/05_Exp3_ContinuousTraining/Trial_04/T0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1675707998425,"user":{"displayName":"Jennifer Feng","userId":"02244244752550378593"},"user_tz":360},"id":"0eeI_ZiCw5Um","outputId":"af61a38b-bbf6-4880-a3a5-0a59b6e2e5e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["# Here define the path, which is used to save the log and trained model in training process\n","\n","# Tminus1_project_name = 'generalizedDet_2022_10_18'\n","\n","# cpu or cuda\n","device ='cpu'\n","if torch.cuda.is_available(): \n","    device='cuda:0'\n","print(device)\n","\n","freqShow = 50\n","weight_reg = 2.0    # balances regression loss with segmentation loss. Value chosen based on past investigation.\n","weight_background = 0.1   # for regression loss only, downweights background pixels to highlight foreground pollen\n","\n","#model parameters\n","batch_size = 4\n","newSize = [800,800] # set to crop size, to tell model what size tensor to expect\n","total_epoch_num = 60  # total number of epoch in training\n","base_lr =0.0005        #0.0005      # base learning rate/\n","scaleList = [0]      # the number of output layer for U-net\n","#scale = [0,1,2,3]      # the number of output layer for U-net"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":922,"status":"ok","timestamp":1675708031833,"user":{"displayName":"Jennifer Feng","userId":"02244244752550378593"},"user_tz":360},"id":"Am-jrOoEw5Uo","outputId":"5abfcae6-03dd-4f0e-b2d9-3d1ddbaf8f20"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(dict_keys(['train_det_list', 'test_det_list']), 240, 60)"]},"metadata":{},"execution_count":7}],"source":["# path_to_image = '/content/gdrive/MyDrive/UTRECHT/Detection/PAL1999/C6/C6_tiles_withAnnot'\n","path_to_image = '/content/gdrive/MyDrive/UTRECHT/Detection/PAL1999'\n","with open(os.path.join(save_dir,'dbinfo.plk'), 'rb') as handle:\n","    dbinfo = pickle.load(handle)    \n","    \n","dbinfo.keys(), len(dbinfo['train_det_list']), len(dbinfo['test_det_list'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6305,"status":"ok","timestamp":1675708048914,"user":{"displayName":"Jennifer Feng","userId":"02244244752550378593"},"user_tz":360},"id":"RrCeZZtJw5Up","outputId":"24112775-572b-4394-c9c2-3f1e36a2178b"},"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","/content/gdrive/MyDrive/exp/05_Exp3_ContinuousTraining/Trial_04/T0/bestValModel_encoder.paramOnly\n"]}],"source":["################## init model ###################\n","# path_to_save_paramOnly = path.join('/content/gdrive/MyDrive/exp/01_Generalized_Model_FullTrainingData/Trial_01','bestValModel_encoder.paramOnly' )\n","path_to_save_paramOnly = path.join(save_dir, 'bestValModel_encoder.paramOnly')  # t_minus_1_save_dir\n","\n","curmodel = PollenDet_SegDistTransform(34, scaleList=scaleList, pretrained=False)\n","curmodel.encoder.encoder.conv1 = nn.Conv2d(27, 64, (7, 7), (2, 2), (3, 3), bias=False) #change dimensions of the first layer in the encoder\n","curmodel.load_state_dict(torch.load(path_to_save_paramOnly)) #, map_location=torch.device('cpu')\n","curmodel.to(device);    \n","#print(curmodel.state_dict)\n","curmodel.eval()\n","#curmodel.train()\n","curmodel.training = False\n","print(curmodel.training)\n","print(path_to_save_paramOnly)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOlT4gd_Vdcb"},"outputs":[],"source":["class PollenDet4Eval(Dataset):\n","    def __init__(self, path_to_image='/content/gdrive/MyDrive/UTRECHT/Detection/PAL1999',\n","                #  path_to_annot='/content/gdrive/MyDrive/UTRECHT/Detection/AnnotCombo9_noNPP_circleMask',\n","                 path_to_mask='/content/gdrive/MyDrive/UTRECHT/Detection/PAL1999',\n","                 dbinfo=dbinfo,\n","                 size=[1040,1392], \n","                 set_name='test'):\n","        \n","        self.path_to_image = path_to_image\n","        # self.path_to_annot = path_to_annot\n","        self.path_to_mask = path_to_mask\n","        self.transform = transform\n","        self.dbinfo = dbinfo\n","        if set_name=='val':\n","            set_name = 'test'\n","        self.set_name = set_name        \n","        self.size = size\n","        self.resizeFactor = size[0]/1000\n","        \n","        self.sampleList = self.dbinfo[set_name+'_det_list']\n","\n","        self.TFNormalize = transforms.Normalize([0.5] * 27, [0.5]*27)\n","        self.current_set_len = len(self.sampleList)\n","        \n","        self.TF2tensor = transforms.ToTensor()\n","        self.TF2PIL = transforms.ToPILImage()\n","        self.TFresize = transforms.Resize((self.size[0],self.size[1]))\n","\n","    def __len__(self):        \n","        return self.current_set_len\n","    \n","    def __getitem__(self, idx):        \n","        current_example= self.sampleList[idx] \n","\n","        current_image_path= os.path.join(self.path_to_image, current_example[0], current_example[0] + '_tiles_withAnnot', current_example[1],current_example[2])\n","        current_distTransform_path=os.path.join(self.path_to_image, current_example[0], current_example[0] + '_masks2', current_example[1],current_example[2])\n","        # curPickleName = path.join(self.path_to_annot, current_example)\n","\n","        imagestack_array = []\n","        for file in sorted(os.listdir(current_image_path)):\n","          if file.endswith('.png'):\n","            slice = Image.open(os.path.join(current_image_path, file))\n","            imagestack_array.append(np.asarray(slice))\n","        image = np.block(imagestack_array)\n","        if image.shape[2] <27:\n","          pad_val = 27-image.shape[2]\n","          npad = ((0, 0), (0,0), (0,pad_val))\n","          image= np.pad(image, pad_width=npad, mode='constant', constant_values=0)\n","\n","        for file in sorted(os.listdir(current_distTransform_path)):\n","          if file.endswith('.png'):\n","            mask = Image.open(os.path.join(current_distTransform_path, file))\n","            mask=np.expand_dims(mask, axis=2)           \n","\n","        label = np.copy(mask)         # 11/30/21 added\n","        label[label > 0] = 1          # 11/30/21 added       \n","\n","        image_label = np.concatenate((image, label, mask), axis=2) #12/13/21 added\n","\n","\n","        mask_distanceTransform = np.copy(mask)     #11/18/21 edited\n","        \n","        labelOrgSize = np.copy(mask)\n","        labelOrgSize = torch.from_numpy(labelOrgSize).unsqueeze(0).unsqueeze(0).squeeze(4)\n","\n","        mask_distanceTransform = mask_distanceTransform.astype(np.float32)/100.0/self.resizeFactor  # factor=size[0]/1000\n","\n","        image = self.TF2tensor(image)\n","        label = torch.from_numpy(label).unsqueeze(0) # self.TF2tensor(label)       \n","        mask_distanceTransform = torch.from_numpy(mask_distanceTransform).unsqueeze(0) # self.TF2tensor(mask_distanceTransform)\n","\n","        image = image.unsqueeze(0)\n","        label = label.unsqueeze(0)        \n","        mask_distanceTransform = mask_distanceTransform.unsqueeze(0)       \n","\n","        height,width,layer = image_label.shape\n","        crop0 = image_label[0:800, 0:800,:]\n","        crop1 = image_label[0:800, width-800:,:]\n","        crop2 = image_label[height-800:, 0:800,:]\n","        crop3 = image_label[height-800:, width-800:,:]\n","\n","        croplist = [crop0, crop1, crop2, crop3]\n","        imgList=[]\n","        labelList=[]\n","        mask_DTList=[]\n","\n","        for idx2 in range(len(croplist)):\n","          image_label = croplist[idx2]\n","          image_label = self.TF2tensor(image_label)\n","          image_label = image_label.unsqueeze(0)  \n","          image = torch.narrow(image_label, 1, 0, image_label.shape[1]-2) \n","          label=torch.narrow(image_label, 1, image_label.shape[1]-2, 1) \n","          mask_distanceTransform=torch.narrow(image_label, 1, image_label.shape[1]-1, 1)\n","\n","          image_label = image_label.squeeze(0)\n","          image = image.type(torch.float)\n","          image = self.TFNormalize(image)\n","\n","          imgList.append(image)\n","          labelList.append(label)\n","          mask_DTList.append(mask_distanceTransform)\n","\n","        image = torch.concat(imgList)\n","        label = torch.concat(labelList)\n","        mask_distanceTransform = torch.concat(mask_DTList)\n","\n","        image = image.squeeze(0)\n","        label = label.squeeze(0)\n","        mask_distanceTransform = mask_distanceTransform.squeeze(0)\n","        labelOrgSize = labelOrgSize.squeeze(0)\n","\n","        return image, label, mask_distanceTransform, labelOrgSize, current_example\n","        # return image, label, mask_distanceTransform, mask_overlap, mask_voteX, mask_voteY, mask_peaks, mask_radius, labelOrgSize, mask_peaksOrgSize, mask_distanceTransformOrgSize, mask_radiusOrgSize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlcosSpxSjC0"},"outputs":[],"source":["set_name = 'test'\n","det_datasets = PollenDet4Eval(path_to_image=path_to_image,\n","                              # path_to_annot=path_to_annotCombo,\n","                              dbinfo=dbinfo, size=newSize, set_name=set_name)\n","\n","dataloaders = DataLoader(det_datasets,\n","                         batch_size=1,\n","                         shuffle=True, \n","                         num_workers=8) # num_work can be set to batch_size\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ho1DmlE4yO6S"},"outputs":[],"source":["def create_circular_mask(mask, center, radius, value=1):\n","    h, w = mask.shape[:2]\n","    Y, X = np.ogrid[:h, :w]\n","    dist_from_center = np.sqrt((Y - center[0])**2 + (X-center[1])**2)\n","\n","    tmpMask = dist_from_center <= radius\n","    mask[tmpMask] = value\n","    \n","    return mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PvqXiRYAaGC"},"outputs":[],"source":["def IOU(box1, box2):\n","    \"\"\"\n","We assume that the box follows the format:\n","box1 = [x1,y1,x2,y2], and box2 = [x3,y3,x4,y4],\n","where (x1,y1) and (x3,y3) represent the top left coordinate,\n","and (x2,y2) and (x4,y4) represent the bottom right coordinate\n","    \"\"\"\n","    x1, y1, x2, y2 = box1\t\n","    x3, y3, x4, y4 = box2\n","    \n","    assert x1 < x2\n","    assert y1 < y2\n","    assert x3 < x4\n","    assert y3 < y4\n","\n","    # determine the coordinates of the intersection rectangle\n","    x_left = max(x1, x3)\n","    y_top = max(y1, y3)\n","    x_right = min(x2, x4)\n","    y_bottom = min(y2, y4)\n","\n","    if x_right < x_left or y_bottom < y_top:\n","        return 0.0\n","\n","    # The intersection of two axis-aligned bounding boxes is always an\n","    # axis-aligned bounding box\n","    intersection_area = (x_right - x_left + 1) * (y_bottom - y_top + 1)\n","    \n","    # compute the area of both AABBs\n","    bb1_area = (x2 - x1) * (y2 - y1)\n","    bb2_area = (x4 - x3) * (y4 - y3)\n","\n","    # compute the intersection over union by taking the intersection\n","    # area and dividing it by the sum of prediction + ground-truth\n","    # areas - the interesection area\n","    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n","    assert iou >= 0.0\n","    assert iou <= 1.0\n","    return iou"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8xBcZh1UlUo7"},"outputs":[],"source":["def nms(boxes, conf_threshold=0.1, iou_threshold=0.5):\n","    bbox_list_thresholded = []\n","    bbox_list_new = []\n","    bbox_list_new_txt = []\n","    # detMask_info = detMask_info\n","\n","    # Stage 1: sort boxes, filter out boxes with low confidence\n","    boxes_sorted = sorted(boxes, reverse=True, key = lambda x : x[1])\n","    for box in boxes_sorted:\n","        if box[1] > conf_threshold:\n","            bbox_list_thresholded.append(box)\n","        else:\n","            pass\n","    # Stage 2: loop through the boxes, remove boxes with high IoU\n","    while len(bbox_list_thresholded) > 0:\n","        current_box = bbox_list_thresholded.pop(0)\n","        bbox_list_new.append(current_box)\n","        current_box_txt = (current_box[0], str(current_box[1]) , str(current_box[2]), str(current_box[3]), str(current_box[4]), str(current_box[5]) )\n","        current_box_txt = ' '.join(current_box_txt)\n","        bbox_list_new_txt.append(current_box_txt)\n","\n","        for box in bbox_list_thresholded:\n","            if current_box[0] == box[0]:\n","                iou = IOU(current_box[2:], box[2:])\n","                # print(iou)\n","                if iou > iou_threshold:\n","                    bbox_list_thresholded.remove(box)\n","                    # detMask_info.remove()\n","\n","    return  bbox_list_new, bbox_list_new_txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0sHB0-VaI_g"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIl_ym-OaIwk"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Q3cmfvnEtrOv"},"source":["# Create text files with bounding box info for evaluation"]},{"cell_type":"code","source":["################## init model ###################\n","path_to_save_paramOnly = path.join(save_dir, 'bestValModel_encoder.paramOnly')  #t_minus_1_save_dir #save_dir\n","\n","curmodel = PollenDet_SegDistTransform(34, scaleList=scaleList, pretrained=False)\n","curmodel.encoder.encoder.conv1 = nn.Conv2d(27, 64, (7, 7), (2, 2), (3, 3), bias=False) #change dimensions of the first layer in the encoder\n","curmodel.load_state_dict(torch.load(path_to_save_paramOnly)) #, map_location=torch.device('cpu')\n","curmodel.to(device);    \n","#print(curmodel.state_dict)\n","curmodel.eval()\n","#curmodel.train()\n","curmodel.training = False\n","print(curmodel.training)\n","print(path_to_save_paramOnly)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6IEe2ZC60pw","executionInfo":{"status":"ok","timestamp":1675708267870,"user_tz":360,"elapsed":1043,"user":{"displayName":"Jennifer Feng","userId":"02244244752550378593"}},"outputId":"94132056-01f6-4700-9449-35667c6698b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","/content/gdrive/MyDrive/exp/05_Exp3_ContinuousTraining/Trial_04/T0/bestValModel_encoder.paramOnly\n"]}]},{"cell_type":"code","source":["# print(project_name)\n","model = [t_minus_1_time_step + '_model',time_step + '_model']"],"metadata":{"id":"wfGz2VZs66IG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVZvHMpGt7LH"},"outputs":[],"source":["detections_dir = os.path.join(eval_dir, model[1], 'det')\n","if not os.path.exists(detections_dir): \n","    os.makedirs(detections_dir)\n","gt_dir = os.path.join(eval_dir, model[1], 'gt')\n","if not os.path.exists(gt_dir): \n","    os.makedirs(gt_dir)"]},{"cell_type":"code","source":["print(detections_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvyUNmkiJcrc","executionInfo":{"status":"ok","timestamp":1675708272637,"user_tz":360,"elapsed":2,"user":{"displayName":"Jennifer Feng","userId":"02244244752550378593"}},"outputId":"802a69de-3228-4811-f154-071895ae5fdc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/eval/05_Exp3_ContinuousTraining/Trial_04/T0/T0_model/det\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWBfFDtq7i5g","executionInfo":{"status":"ok","timestamp":1675708340621,"user_tz":360,"elapsed":60386,"user":{"displayName":"Jennifer Feng","userId":"02244244752550378593"}},"outputId":"87c8d44e-7989-4b4c-d734-b2c943989427"},"outputs":[{"output_type":"stream","name":"stdout","text":["50/60\n"]}],"source":["iterCount, sampleCount = 0, 0\n","phase = 'test' # 'train'\n","for sample in dataloaders: \n","            \n","    curImg, curLabel, curMask, curMask_orgSize, current_example = sample\n","    \n","    curImg, curLabel, curMask, curMask_orgSize = curImg.to(device), curLabel.to(device), curMask.to(device), curMask_orgSize.to(device)\n","\n","    curImg_squeeze=torch.squeeze(curImg,0)\n","\n","    iterCount += 1\n","    sampleCount += curLabel.size(0)   \n","    \n","    outputs = curmodel(curImg_squeeze)\n","    predSeg = outputs[('segMask', 0)]\n","    predDistTransform = outputs[('output', 0)]\n","\n","    softmax = predSeg\n","    \n","    if iterCount%50==0:\n","        print('{}/{}'.format(iterCount,len(det_datasets)))\n","\n","    #######################################################\n","    ##       ground-truth: labelOrgSize, mask_peaksOrgSize\n","    #######################################################\n","    curmask_OrgSize = curMask_orgSize.squeeze().cpu().detach().numpy()\n","    GTSegMask = curmask_OrgSize >0\n","    GTSegMask = measure.label(GTSegMask, background=0)\n","\n","    props = measure.regionprops(np.squeeze(GTSegMask)) #get the properties of the connected components\n","    bbox = [prop.bbox for prop in props]   #bounding box coordinates for connected components\n","    gt_filename = os.path.join(gt_dir, current_example[0][0] + '_' + current_example[1][0] + '_' + current_example[2][0] + '.txt')\n","    # i=2\n","    gt_info = []\n","    for i in range(len(props)): \n","      class_name=\"finetuned\"   #det\n","      left = bbox[i][1]\n","      top = bbox[i][0]\n","      right = bbox[i][3]\n","      bottom = bbox[i][2]\n","      # predDistTransformCrop = predDistTransform[top:bottom, left:right]\n","      # confidence = np.amax(predDistTransformCrop)\n","      bbox_info = [class_name, str(left), str(top), str(right), str(bottom)]\n","      bbox_info = ' '.join(bbox_info)\n","\n","      gt_info.append(bbox_info)\n","\n","    # fn = open(gt_filename,'w')\n","    fn = open(gt_filename,'a')\n","    for i in gt_info:\n","      fn.write(i + \"\\n\")\n","    fn.close()\n","\n","    ##############################################\n","    ##          prediction: \n","    ##############################################\n","    # create a list of (800x800) prediction distance transforms crops and softmax crops\n","\n","    predDistTransform_crops=[]\n","    softmax_crops = []\n","\n","    for idx in range(0,4):\n","      tmpImg =  predDistTransform[idx,:,:,:].squeeze().cpu().detach().numpy() \n","      predDistTransform_crops.append(tmpImg)\n","\n","    for idx in range(0,4):\n","      tmpImg =  softmax[idx,:,:,:].squeeze().cpu().detach().numpy() \n","      softmax_crops.append(tmpImg)\n","\n","    # create full-sized pred distance transform \n","    mask_OrgSize = curMask_orgSize.squeeze().cpu().detach().numpy()\n","\n","    height,width = mask_OrgSize.shape\n","    predDistTransform=np.zeros_like(mask_OrgSize)  \n","    predDistTransform=predDistTransform.astype(np.float32)\n","\n","    tmp_predDistTransform_1=np.zeros_like(mask_OrgSize).astype(np.float32)\n","    tmp_predDistTransform_2=np.zeros_like(mask_OrgSize).astype(np.float32)\n","    tmp_predDistTransform_3=np.zeros_like(mask_OrgSize).astype(np.float32)\n","    tmp_predDistTransform_4=np.zeros_like(mask_OrgSize).astype(np.float32)\n","\n","\n","    tmp_predDistTransform_1[0:800, 0:800]=predDistTransform_crops[0]\n","    tmp_predDistTransform_2[0:800, width-800:]=predDistTransform_crops[1]\n","    tmp_predDistTransform_3[height-800:, 0:800]=predDistTransform_crops[2]\n","    tmp_predDistTransform_4[height-800:, width-800:]=predDistTransform_crops[3]\n","\n","    predDistTransform = np.maximum.reduce([tmp_predDistTransform_1,tmp_predDistTransform_2,tmp_predDistTransform_3,tmp_predDistTransform_4]) \n","    predDistTransform = gaussian_filter(predDistTransform, sigma=10) # gaussian blur to get rid of shadow\n","    pred_distanceTransform = np.copy(predDistTransform)\n","\n","    # create full-sized softmax\n","    height,width = mask_OrgSize.shape\n","    softmax=np.zeros_like(mask_OrgSize)  \n","    softmax=softmax.astype(np.float32)\n","\n","    tmp_softmax_1=np.zeros_like(mask_OrgSize).astype(np.float32)\n","    tmp_softmax_2=np.zeros_like(mask_OrgSize).astype(np.float32)\n","    tmp_softmax_3=np.zeros_like(mask_OrgSize).astype(np.float32)\n","    tmp_softmax_4=np.zeros_like(mask_OrgSize).astype(np.float32)\n","\n","    tmp_softmax_1[0:800, 0:800]=softmax_crops[0]\n","    tmp_softmax_2[0:800, width-800:]=softmax_crops[1]\n","    tmp_softmax_3[height-800:, 0:800]=softmax_crops[2]\n","    tmp_softmax_4[height-800:, width-800:]=softmax_crops[3]\n","\n","    tmp_softmax_1[tmp_softmax_1 == 0] = np.nan\n","    tmp_softmax_2[tmp_softmax_2 == 0] = np.nan\n","    tmp_softmax_3[tmp_softmax_3 == 0] = np.nan\n","    tmp_softmax_4[tmp_softmax_4 == 0] = np.nan\n","\n","    # softmax = np.maximum.reduce([tmp_softmax_1,tmp_softmax_2,tmp_softmax_3,tmp_softmax_4]) \n","    softmax =  np.nanmean(np.array([tmp_softmax_1,tmp_softmax_2,tmp_softmax_3,tmp_softmax_4]), axis=0)\n","\n","    # find peaks, zero-out background noise\n","    voting4center = np.copy(pred_distanceTransform)\n","    voting4center[voting4center<0.001] = 0\n","    coord_peaks = feature.peak_local_max(voting4center, min_distance=50, exclude_border=False) #originally min_distance =5, changed to 25\n","\n","    # create detection mask using peaks and predicted radius\n","    detMask = voting4center*0\n","    predRadiusList = []\n","    size = (400,400)\n","    detection_info = []\n","    detection_info2 = []\n","    det_filename = os.path.join(detections_dir, current_example[0][0] + '_' + current_example[1][0] + '_' + current_example[2][0]+ '.txt')\n","\n","\n","    for i in range(coord_peaks.shape[0]):\n","        y, x = coord_peaks[i]\n","        #centerMask[y, x] = 1\n","        # centerMask[y-10:y+10, x-10:x+10] = 1\n","        # predRadiusList += [voting4center[y,x] *100*800/1000 * 800/1000] # /100.0/self.resizeFactor      # why smaller?\n","        # predRadiusList += [voting4center[y,x] *100*1000/1000 * 1000/1000]\n","\n","        left = int(x-(size[0]/2))\n","        left=max(left,0)\n","        top = int(y-(size[0]/2))\n","        top=max(top,0)\n","        right = int(x+(size[0]/2))\n","        right=max(right,0)\n","        bottom = int(y+(size[0]/2))\n","        bottom=max(bottom,0)\n","\n","        tmpCrop = softmax[top:bottom, left:right]\n","        thresh = threshold_otsu(tmpCrop) \n","        tmpCrop = tmpCrop> thresh # binarize\n","        tmpCrop= measure.label(tmpCrop, background=0)\n","        props = measure.regionprops(tmpCrop) #get the properties of the connected components\n","\n","        # diameter = [prop.feret_diameter_max for prop in props]\n","        # diameter = [prop.equivalent_diameter for prop in props]  #diameter for connected components\n","        diameter = [prop.major_axis_length for prop in props]   #diameter for connected components\n","        if len(diameter) !=0 and max(diameter)!=0:\n","          radius = int(max(diameter)/2)\n","          predRadiusList += [radius]\n","          a = len(predRadiusList)-1\n","\n","          tmpMask = voting4center*0\n","          tmpMask = create_circular_mask(tmpMask, [y, x], predRadiusList[a], value=1)\n","\n","          #write bb coords to file\n","          # bbox = [prop.bbox for prop in props]   #bounding box coordinates for connected components\n","\n","          class_name=\"finetuned\"    #det\n","          leftBb = x-radius\n","          leftBb = max(leftBb,0)\n","          topBb = y-radius\n","          topBb= max(topBb,0)\n","          rightBb = x+radius\n","          rightBb = min(rightBb,tmpMask.shape[1] )\n","          bottomBb = y+radius\n","          bottomBb = min(bottomBb,tmpMask.shape[0])\n","\n","          masked_softmax = np.ma.masked_where(tmpMask==0, softmax) \n","          confidence = np.nanmean(masked_softmax)\n","          # if confidence > 0.99:\n","          #   confidence = 0\n","          # bbox_info = [class_name,str(confidence) , str(leftBb), str(topBb), str(rightBb), str(bottomBb)]\n","          bbox_info2 = [class_name, confidence, leftBb, topBb, rightBb, bottomBb]\n","          detection_info2.append(bbox_info2)\n","\n","    # Apply non-max suppression\n","    NMS_bb = nms(detection_info2,conf_threshold=0, iou_threshold=0.3)\n","    NMS_bb = NMS_bb[1]\n","\n","\n","    # fn = open(det_filename,'w')\n","    fn = open(det_filename,'a')\n","    for i in NMS_bb:\n","      fn.write(i + \"\\n\")\n","    fn.close()"]},{"cell_type":"code","source":[],"metadata":{"id":"yP176jJYQCOL"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"11kX06MaXNoPtsXtPzqwFKOxKq-QkiU3J","timestamp":1643318796487},{"file_id":"19scDWQ4L79clhNFzHCUF6p4dOSg71kPZ","timestamp":1643243143306},{"file_id":"1LBR2EGgGSmP8F16tB17lU0udI9aGJOVE","timestamp":1642701132152},{"file_id":"1pbz8JGQbXgBsY0i68W2hbMnxv-HZrDzc","timestamp":1641538196383},{"file_id":"1Bf1dJz-tiEa39pj7_5xq6sIqy5sav7tG","timestamp":1641252892862},{"file_id":"1hWiEAazHjKMSjYgci-ZM-4RiRsl-xulx","timestamp":1639780045728},{"file_id":"1dFMFlSD7JumeBNAxDC1GO5I2vY_puom-","timestamp":1639751288360},{"file_id":"1_cGNWaPlb7RGYHbK0p5up6USl8Jqy5l_","timestamp":1638377383878},{"file_id":"1Jovw-ecDYWmrl2byWTMTtfH17w2p_rw1","timestamp":1636216349035},{"file_id":"13s6xpTJ8dA5aUQn1FjgYBwPhfJKO21cM","timestamp":1635990836913},{"file_id":"1wMpFpk61uyI6XoMQjnIvpuaCmVYCRmg6","timestamp":1633643078356}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}